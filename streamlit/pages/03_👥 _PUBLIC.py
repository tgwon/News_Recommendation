#ë¼ì´ë¸ŒëŸ¬ë¦¬ import
#í•„ìš”í•œ ê²½ìš° install
import streamlit as st
from streamlit import components
from keybert import KeyBERT
import matplotlib.pyplot as plt
import plotly
import plotly.express as px
import seaborn as sns
import numpy as np
from numpy.linalg import norm
from numpy import nan
from numpy import dot
import ast
from PIL import Image
import pandas as pd
import time
import random
from konlpy.tag import Twitter
from konlpy.tag import Okt
from collections import Counter
from wordcloud import WordCloud
import re
import math
from sklearn.preprocessing import normalize
from konlpy.tag import Komoran

#LDAë¥¼ ìœ„í•œ íŒ¨í‚¤ì§€
from gensim import corpora
import gensim
import pyLDAvis
import pyLDAvis.gensim

#plotly ì‹œê°í™” ì˜¤ë¥˜ì‹œ ì‹¤í–‰ì‹œí‚¬ ì½”ë“œ
#import plotly.offline as pyo
#import plotly.graph_objs as go
# ì˜¤í”„ë¼ì¸ ëª¨ë“œë¡œ ë³€ê²½í•˜ê¸°
#pyo.init_notebook_mode()

#public í˜ì´ì§€ë¥¼ ìœ„í•œ ì½”ë“œ
st.set_page_config(page_title="PUBLIC", page_icon="ğŸ‘¥",layout = 'wide')

image = Image.open('images/logo.png')
image2 = Image.open('images/logo2.png')
image3 = Image.open('images/logo3.png')

st.image(image, width=120)
st.sidebar.image(image2, use_column_width=True)
st.sidebar.image(image3, use_column_width=True)

#ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ì¸ì‹í•˜ëŠ” ë¬¸ì œ í•´ê²°í•˜ëŠ” í•¨ìˆ˜
def parse_list(input_str):

    return eval(input_str)


# í™”ë©´ì´ ì—…ë°ì´íŠ¸ë  ë•Œ ë§ˆë‹¤ ë³€ìˆ˜ í• ë‹¹ì´ ëœë‹¤ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë ¤ì„œ @st.cache_data ì‚¬ìš©(ìºì‹±)
@st.cache_data
def load_client_fv_data():

    #ê³ ê° Feature Vector
    client_fv = pd.read_csv("data/client_feature_vector.csv", converters={'feature': parse_list})

    return client_fv

client_fv = load_client_fv_data()


@st.cache_data
def daily_result_load_data():

    #daily news ì „ì²˜ë¦¬ ë° ëª¨ë¸ë§ ê²°ê³¼
    daily_result = pd.read_csv("data/daily_result.csv", converters={'fv': parse_list})

    return daily_result

daily_result = daily_result_load_data()


@st.cache_data
def lda():
    tokenized_doc = daily_result['content_lda'].apply(lambda x: x.split())
    dictionary = corpora.Dictionary(tokenized_doc)
    corpus = [dictionary.doc2bow(text) for text in tokenized_doc]
    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 5,
                                           id2word=dictionary,
                                           passes=15)
    topics = ldamodel.print_topics(num_words=4)

    #pyLDAvis.enable_notebook() -> .py íŒŒì¼ì—ì„œëŠ” ì•ˆë¨


    #ì¸í„°ë„· ì—°ê²° ì•ˆë˜ë©´ streamlit ìƒì—ì„œ ì•ˆë³´ì„
    vis =  pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)

    #htmlìœ¼ë¡œ ì‹œê°í™”
    html_string = pyLDAvis.prepared_data_to_html(vis)

    return html_string

html_string = lda()


st.markdown('''
<h2>Daily Report For <span style="color: #6FA8DC;"> EVERYONE ğŸ‘¥</span></h2>
''', unsafe_allow_html=True)
st.text('')
########################################################################################################################################
#st.columns í•¨ìˆ˜ë¡œ í™”ë©´ ë ˆì´ì•„ì›ƒì„ ì—´ë¡œ ë¶„ë¦¬
col1, col2, col3 = st.columns([4,0.5,5.5])

with col1:
    #ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ í˜„í™©
    st.write('')
    value_counts = daily_result['category'].value_counts()
    fig2=plt.figure(figsize=(10,5))
    plt.rcParams['font.family'] = 'HYdnkM'
    value_counts.plot(kind='bar', color='skyblue')
    plt.title('ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ í˜„í™©', fontsize=25)
    plt.rc('xtick', labelsize=14) 
    plt.xlabel('')
    plt.xticks(rotation=0)
    for i, j in zip([0,1,2,3,4,5,6,7], value_counts.values):
        plt.text(i, j, f'{int(j/len(daily_result)*100)}%', ha='center', va='bottom')
    st.pyplot(fig2)

with col2:
    st.write('')

with col3:
    #ì˜¤ëŠ˜ì˜ ì†Œë¹„ì ë‹¨ì–´
    cw = pd.read_csv("data/consumer_word.csv")
    num_indices = len(cw)
    # ëœë¤ìœ¼ë¡œ ì¸ë±ìŠ¤ 3ê°œ ì¶”ì¶œ
    random_indices = random.sample(range(num_indices), 3)
    st.subheader("ğŸ“š ì˜¤ëŠ˜ì˜ ì†Œë¹„ì ë‹¨ì–´")
    st.write('')
    st.write(f'**"{cw.ë‹¨ì–´[random_indices[0]]}"**')
    st.write(f'**ì˜ë¯¸ : {cw.ëœ»[random_indices[0]]}**')

    # linktree í˜•ì‹
    st.markdown(
        """
    <style>
    .link-card {
        display: flex;
        flex-direction: column;
        padding: 5px;
        margin-bottom: 10px;
        border: 1px solid #E8DDDA;
        border-radius: 15px;
        background-color: white; 
        box-shadow: 0px 0px 5px #F4EDEC;
    }
    .link-card:hover {
    background-color: #FFF6F3;
    }
    a {
        color: black!important;
        text-decoration: none!important;
    }
    </style>
    """,
        unsafe_allow_html=True,
    )

    #í•˜ì´í¼ë§í¬ ë§Œë“œëŠ” í•¨ìˆ˜
    def create_link_card(title, url):
        container = st.container()
        container.markdown(
            f'<div class="link-card"><a href="{url}" target="_blank">{title}</a></div>',
            unsafe_allow_html=True,
        )
        return container
    
    create_link_card(
        "ğŸ’¡ë„¤ì´ë²„ ì§€ì‹ë°±ê³¼ë¡œ ë°”ë¡œê°€ê¸°",
        cw.url[random_indices[0]],
    )
########################################################################################################################################
st.write('')
st.write('')
st.subheader("ğŸ‘€ ë‰´ìŠ¤ í† í”½ ì•Œì•„ë³´ê¸°")

#LDA ì‹œê°í™”
components.v1.html(html_string, width=1320, height=880, scrolling=True)
########################################################################################################################################
st.write('')
st.subheader('ğŸ‘‡ ì¹´í…Œê³ ë¦¬ë³„ ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ë¥¼ í™•ì¸í•˜ì„¸ìš”')

#8ê°œì˜ íƒ­ì— ëŒ€í•´ì„œ ë™ì¼í•œ ì½”ë“œ ì ìš©
tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs(['**ì—¬í–‰** âœˆ', 
                                                            '**ì·¨ë¯¸**  ğŸŸ',
                                                            '**IT_ì „ì** ğŸ’»',
                                                            '**ìƒí™œ**  ğŸª',
                                                            '**íŒ¨ì…˜_ë·°í‹°**  ğŸ‘•',
                                                            '**êµìœ¡**  ğŸ“–',
                                                            '**ì˜ë£Œ**  ğŸ©º',
                                                            '**ì™¸ì‹**  ğŸ£'])
with tab1:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**ì—¬í–‰** âœˆ')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[0])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))
                
                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################
with tab2:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**ì·¨ë¯¸**  ğŸŸ')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[1])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################
with tab3:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**IT_ì „ì** ğŸ’»')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[2])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

with tab4:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**ìƒí™œ**  ğŸª')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[3])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

with tab5:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**íŒ¨ì…˜_ë·°í‹°**  ğŸ‘•')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[4])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

with tab6:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**êµìœ¡**  ğŸ“–')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[5])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
##################################################################################################

with tab7:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**ì˜ë£Œ**  ğŸ©º')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[6])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
################################################################################################## 

with tab8:
    col1, col2 = st.columns([5.5,4.5])
    with col1:
        st.subheader('**ì™¸ì‹**  ğŸ£')
        con1 = st.container()
        con1.caption("ğŸ’¡ ì œëª©ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
        #index ì°¾ê¸°
        max_first_index = daily_result['fv'].apply(lambda x: x[7])
        max_indices = max_first_index.nlargest(3).index.tolist()
        if st.button(f'"**{daily_result.title[max_indices[0]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[0]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[0]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[0]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[0]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[0]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[1]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[1]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[1]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[1]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[1]])
##################################################################################################
        if st.button(f'"**{daily_result.title[max_indices[2]]}**"'):
            with col2:
#################################################################################################
                st.subheader("ğŸ‘€ ì‹œê°í™”")
                okt = Okt()
                nouns = okt.nouns(daily_result.content_copy[max_indices[2]])
                #stop_words = ""
                #stop_words = set(stop_words.split(' '))

                words = [n for n in nouns if len(n) > 1] # ë‹¨ì–´ì˜ ê¸¸ì´ê°€ 1ê°œì¸ ê²ƒì€ ì œì™¸

                c = Counter(words) # ìœ„ì—ì„œ ì–»ì€ wordsë¥¼ ì²˜ë¦¬í•˜ì—¬ ë‹¨ì–´ë³„ ë¹ˆë„ìˆ˜ í˜•íƒœì˜ ë”•ì…”ë„ˆë¦¬ ë°ì´í„°ë¥¼ êµ¬ì„±

                wordcloud = WordCloud(
                    font_path = 'malgun.ttf',
                    background_color='white', 
                    colormap='Blues' 
                ).generate_from_frequencies(c)

                fig1 = plt.figure()
                #plt.figure(figsize=(10,10))
                plt.imshow(wordcloud,interpolation='bilinear')
                plt.axis('off')
                plt.show()
                st.pyplot(fig1)               
#################################################################################################
                st.subheader("ğŸ“ ìš”ì•½")

                text = re.sub(r"\n+", " ", daily_result.content_copy[max_indices[2]])

                sentences = re.split(r'[.!?]+\s*|\n+', daily_result.content_copy[max_indices[1]])

                data = []
                for sentence in sentences:
                    if(sentence == "" or len(sentence) == 0):
                        continue
                    temp_dict = dict()
                    temp_dict['sentence'] = sentence
                    temp_dict['token_list'] = sentence.split() #ê°€ì¥ ê¸°ì´ˆì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì!

                    data.append(temp_dict)

                df = pd.DataFrame(data)

                
                # sentence similarity = len(intersection) / log(len(set_A)) + log(len(set_B))
                similarity_matrix = []
                for i, row_i in df.iterrows():
                    i_row_vec = []
                    for j, row_j in df.iterrows():
                        if i == j:
                            i_row_vec.append(0.0)
                        else:
                            intersection = len(set(row_i['token_list']) & set(row_j['token_list']))
                            log_i = math.log(len(set(row_i['token_list']))) if len(set(row_i['token_list'])) > 0 else 0
                            log_j = math.log(len(set(row_j['token_list']))) if len(set(row_j['token_list'])) > 0 else 0
                            
                            if log_i == 0 or log_j == 0:
                                similarity = 0
                            else:
                                similarity = intersection / (log_i + log_j)
                            
                            i_row_vec.append(similarity)
                    similarity_matrix.append(i_row_vec)

                weightGraph = np.array(similarity_matrix)
                
                def pagerank(x, df=0.85, max_iter=30):
                    assert 0 < df < 1

                    # initialize
                    A = normalize(x, axis=0, norm='l1')
                    R = np.ones(A.shape[0]).reshape(-1,1)
                    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1,1)
                    # iteration
                    for _ in range(max_iter):
                        R = df * (A * R) + bias

                    return R

                
                R = pagerank(weightGraph) # pagerankë¥¼ ëŒë ¤ì„œ rank matrix ë°˜í™˜
                R = R.sum(axis=1) # ë°˜í™˜ëœ matrixë¥¼ row ë³„ë¡œ sum
                indexs = R.argsort()[-3:] # í•´ë‹¹ rank ê°’ì„ sort, ê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ indexë¥¼ ë°˜í™˜


                #rankê°’ì´ ë†’ì€ 3ê°œì˜ ë¬¸ì¥ ì¶œë ¥
                for index in sorted(indexs):
                    st.write(df['sentence'][index])
                
                st.write('')
#################################################################################################
                st.subheader("ğŸ”‘ í‚¤ì›Œë“œ")
                kw_model = KeyBERT()
                n=3 #í‚¤ì›Œë“œ 3ê°œ
                keywords_mmr = kw_model.extract_keywords(daily_result.content[max_indices[2]],keyphrase_ngram_range=(1,1),use_mmr = True,top_n = n, diversity = 0.2)
                st.write('#'+keywords_mmr[0][0],' ', '#'+keywords_mmr[1][0],' ', '#'+keywords_mmr[2][0])
                st.write('')
##################################################################################################
                st.subheader('ğŸ“° ì›ë¬¸ ë§í¬')
                st.write(daily_result.url[max_indices[2]])
##################################################################################################
##################################################################################################
##################################################################################################
################################################################################################## 
